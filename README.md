# Experiment on using hebbian learning (1 Winner-Takes-All) as weights initialization method on CIFAR10 for speedup training

### Hebbian:
1. Default weights initialization accuracy: 8.16%
2. Hebbian update: 29.63%
3. Single epoch SGD backprop: 53.73%

### No hebbian:
1. Default weights initialization accuracy: 9.47%
2. Hebbian update: -
3. Single epoch SGD backprop: 53.27%

## Conclusion:
Nope